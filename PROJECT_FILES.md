# ğŸ“ Project Files & Configuration Guide

Complete reference for all files, scripts, and configuration in the Agent-Driven TODO Executor project.

## Table of Contents
1. [Shell Scripts](#shell-scripts)
2. [Configuration Files](#configuration-files)
3. [Data Files](#data-files)
4. [Project Structure](#project-structure)
5. [Usage Guide](#usage-guide)

---

## ğŸ”§ Shell Scripts

### start_server.sh
**Purpose**: Quick startup script for the web application

**Location**: `/start_server.sh`

**Contents**:
```bash
#!/bin/bash
# Start the todo-ai-agent web server with API key configured

set -a
source .env 2>/dev/null || echo "Note: .env file not found, using OPENAI_API_KEY from environment"
set +a

cd "$(dirname "$0")" || exit
source .venv/bin/activate
python -m uvicorn server.app:app --host 127.0.0.1 --port 8000
```

**What it does**:
- Loads environment variables from `.env` file if it exists
- Activates Python virtual environment
- Starts FastAPI server on localhost:8000

**How to use**:
```bash
# Make it executable (first time only)
chmod +x start_server.sh

# Run the script
./start_server.sh
```

**Expected output**:
```
[planner.py] Python executable: /path/to/.venv/bin/python
[startup] Database initialized
INFO:     Uvicorn running on http://127.0.0.1:8000
```

**Requirements**:
- Python virtual environment (`.venv/`) must exist
- OpenAI API key set in `.env` or environment
- All dependencies installed via `pip install -r requirements.txt`

---

### demo.sh
**Purpose**: Interactive demonstration of all system features

**Location**: `/demo.sh`

**Size**: 246 lines of comprehensive demo code

**Features**:
- ğŸ¨ Colored terminal output for readability
- ğŸ” Environment validation (checks for .venv, dependencies)
- ğŸš€ Automatic server startup and shutdown
- ğŸ“Š Multiple demo scenarios testing different workflows
- ğŸ“ˆ System metrics display
- ğŸŒ Opens web UI in browser
- âŒ¨ï¸ Tests CLI interface
- ğŸ›¡ï¸ Automatic cleanup on exit with trap handler

**Key Functions**:
- `check_environment()` - Validates setup
- `start_server()` - Starts FastAPI server
- `run_demo_scenarios()` - Executes example workflows
- `show_metrics()` - Displays system stats
- `show_web_ui()` - Opens browser to web interface
- `test_cli()` - Tests command-line interface
- `cleanup()` - Stops server on exit

**How to use**:
```bash
# Make it executable (first time only)
chmod +x demo.sh

# Run the demo
./demo.sh
```

**What it demonstrates**:
```
âœ… Intelligent goal planning with LLM
âœ… Real-time streaming progress updates
âœ… Interactive confirmation mode
âœ… Circuit breaker for fault tolerance
âœ… Undo functionality with state rollback
âœ… Modern web UI with theme toggle
âœ… Comprehensive metrics and monitoring
âœ… CLI and API interfaces
âœ… Error handling and recovery
âœ… Production-grade reliability features
```

**Requirements**:
- Same as start_server.sh
- Browser installed (for showing web UI)

---

## âš™ï¸ Configuration Files

### .env (Environment Configuration)
**Purpose**: Store sensitive configuration like API keys

**Location**: `/.env`

**Template Location**: `/.env.example`

**Contents**:
```dotenv
# OpenAI API Configuration
# Get your API key from: https://platform.openai.com/account/api-keys
OPENAI_API_KEY=sk-proj-your-actual-key-here

# Optional: Set default model (defaults to gpt-4o if not specified)
# OPENAI_MODEL=gpt-4o
```

**How to create it**:
```bash
# Method 1: Copy from template
cp .env.example .env
nano .env  # Edit with your API key

# Method 2: Create manually
echo "OPENAI_API_KEY=sk-proj-your-key-here" > .env
```

**What each variable does**:
- `OPENAI_API_KEY` (required)
  - Your OpenAI API key
  - Get from https://platform.openai.com/api-keys
  - Never commit to Git (it's in .gitignore)
  - Used by planner and executor tools

- `OPENAI_MODEL` (optional)
  - Default LLM model to use
  - Defaults to gpt-4o if not set
  - Can be overridden per run in web UI

**âš ï¸ Security Notes**:
- **Never share your .env file** - contains API credentials
- **Never commit to Git** - already in .gitignore
- **Never hard-code API keys** - always use .env
- **Rotate keys regularly** - best practice for security

**Alternative: Set via environment**:
```bash
# Instead of .env file, you can set environment variable
export OPENAI_API_KEY="sk-proj-your-key"
./start_server.sh
```

---

### .env.example (Configuration Template)
**Purpose**: Template showing all possible environment variables

**Location**: `/.env.example`

**Contents**:
```dotenv
# OpenAI API Configuration
# Get your API key from: https://platform.openai.com/account/api-keys
OPENAI_API_KEY=sk-proj-your-api-key-here

# Optional: Set default model (defaults to gpt-4o if not specified)
# OPENAI_MODEL=gpt-4o
```

**How to use**:
- Reference this file to see all available configuration options
- Copy it to `.env` and fill in your actual values
- Never modify this file for your credentials
- This file is safe to commit (doesn't have real credentials)

---

### .gitignore (Git Ignore Configuration)
**Purpose**: Tell Git which files NOT to commit

**Typically ignores**:
- `.env` - Contains API credentials
- `.venv/` - Virtual environment (can be recreated)
- `__pycache__/` - Python bytecode
- `*.pyc` - Compiled Python
- `.DS_Store` - macOS metadata
- `node_modules/` - JavaScript dependencies (if used)
- IDE files (`.vscode/`, `.idea/`)

**Why it matters**:
- Prevents accidental credential leaks
- Keeps repository clean
- Prevents conflicts from environment-specific files

---

## ğŸ’¾ Data Files

### state.json (CLI Execution History)
**Purpose**: Persistent storage of agent execution state for CLI interface

**Location**: `/state.json`

**Created by**: `python -m agent.runner --mode confirm --persist`

**Structure**:
```json
{
  "tasks": [
    {
      "id": 1,
      "title": "Design Landing Page Layout",
      "description": "Create a visually appealing and user-friendly layout for the landing page.",
      "status": "done"
    },
    {
      "id": 2,
      "title": "Develop Email Capture Form",
      "description": "Implement a form to collect visitor email addresses securely.",
      "status": "done"
    }
  ],
  "trace": [
    {
      "task_id": 1,
      "title": "Define Landing Page Structure",
      "result": "done",
      "reflection": "The landing page structure was effectively defined, ensuring a clear organization of essential sections..."
    },
    {
      "task_id": 2,
      "title": "Design Visual Elements",
      "result": "done",
      "reflection": "The design for the landing page was successfully completed, effectively incorporating a cohesive color scheme..."
    }
  ]
}
```

**Field Descriptions**:

**tasks array**:
- `id` (number) - Unique task identifier
- `title` (string) - Task name/title
- `description` (string) - What the task does
- `status` (string) - Current state:
  - `done` - Task completed successfully
  - `in-progress` - Task currently executing
  - `failed` - Task execution failed
  - `not-started` - Task hasn't started

**trace array**:
- `task_id` (number) - Which task this trace belongs to
- `title` (string) - Task being executed
- `result` (string) - Outcome (done, failed, etc.)
- `reflection` (string) - AI-generated summary of task execution (1-2 sentences)
  - Generated by LLM after each task
  - Explains what was accomplished or what went wrong
  - Useful for understanding task results

**When it's created**:
```bash
# CLI with persist flag - saves to state.json
python -m agent.runner --mode confirm --persist

# Web interface - saves to SQLite database instead
./start_server.sh
```

**How it's used**:
1. **State persistence**: Resume interrupted runs
2. **Audit trail**: Complete history of executions
3. **Debugging**: LLM reflections help understand task outcomes
4. **Recovery**: Restore previous state if needed

**Example workflow**:
```bash
# Start with persist
$ python -m agent.runner --persist
Goal: Create a landing page
Mode: Confirm
[Task 1 completes, saves to state.json]
[Task 2 completes, saves to state.json]
[User interrupts with Ctrl+C]

# Later, run again - state.json is loaded
$ python -m agent.runner --persist
[Previous tasks are already in state.json]
[Can resume from where it left off]
```

**Editing state.json**:
- You can manually edit if needed (it's just JSON)
- Useful for resetting failed tasks or testing
- Changes take effect on next run with `--persist`

---

### agent_runs.db (Web Run History Database)
**Purpose**: SQLite database storing web interface run history

**Location**: `/agent_runs.db`

**Created by**: First run of `./start_server.sh`

**Auto-created schema**:
```sql
-- Stores run information
CREATE TABLE runs (
  id TEXT PRIMARY KEY,
  goal TEXT,
  model TEXT,
  mode TEXT,
  status TEXT,
  created_at TIMESTAMP,
  completed_at TIMESTAMP
)

-- Stores tasks for each run
CREATE TABLE tasks (
  id TEXT,
  run_id TEXT,
  task_id TEXT,
  title TEXT,
  description TEXT,
  status TEXT,
  FOREIGN KEY (run_id) REFERENCES runs(id)
)

-- Stores event stream for real-time updates
CREATE TABLE events (
  id INTEGER PRIMARY KEY,
  run_id TEXT,
  event_type TEXT,
  data TEXT,
  timestamp TIMESTAMP,
  FOREIGN KEY (run_id) REFERENCES runs(id)
)
```

**What it stores**:
- **runs**: Run metadata (ID, goal, model, status)
- **tasks**: Task details for each run
- **events**: Event stream for real-time updates

**How to access**:
```bash
# View all runs
curl http://127.0.0.1:8000/runs

# View specific run details
curl http://127.0.0.1:8000/run/{run_id}
```

**To reset/delete**:
```bash
# Remove the database file
rm agent_runs.db

# Restart server - it creates a new empty database
./start_server.sh
```

---

## ğŸ“‚ Project Structure

### Complete Directory Layout
```
todo-ai-agent/
â”œâ”€â”€ ğŸ“„ README.md                    # Main project documentation â­
â”œâ”€â”€ ğŸ“„ RUNNING_INSTRUCTIONS.md      # Complete setup guide â­
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md              # System design and deployment
â”œâ”€â”€ ğŸ“„ FEATURES.md                  # Feature showcase
â”œâ”€â”€ ğŸ“„ API.md                       # API reference
â”œâ”€â”€ ğŸ“„ PROJECT_FILES.md             # This file
â”‚
â”œâ”€â”€ ğŸ”§ Configuration & Scripts
â”‚   â”œâ”€â”€ .env                        # Environment variables (create from .env.example)
â”‚   â”œâ”€â”€ .env.example                # API key template
â”‚   â”œâ”€â”€ start_server.sh             # Quick server startup
â”‚   â”œâ”€â”€ demo.sh                     # Interactive demo
â”‚   â”œâ”€â”€ .gitignore                  # Git ignore rules
â”‚   â””â”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ ğŸ’¾ Data Files
â”‚   â”œâ”€â”€ state.json                  # CLI execution history
â”‚   â””â”€â”€ agent_runs.db               # Web run history (SQLite)
â”‚
â”œâ”€â”€ ğŸ Python Packages
â”‚   â”œâ”€â”€ agent/                      # Core agent logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent_runner.py         # Main agent orchestrator
â”‚   â”‚   â”œâ”€â”€ agent_tools.py          # LangChain tool wrappers
â”‚   â”‚   â”œâ”€â”€ executor.py             # Task execution engine
â”‚   â”‚   â”œâ”€â”€ persistence.py          # JSON state management
â”‚   â”‚   â”œâ”€â”€ planner.py              # LLM-powered planning
â”‚   â”‚   â”œâ”€â”€ runner.py               # CLI interface
â”‚   â”‚   â”œâ”€â”€ ui.py                   # Rich TUI components
â”‚   â”‚   â””â”€â”€ __pycache__/            # Compiled Python bytecode
â”‚   â”‚
â”‚   â””â”€â”€ server/                     # Web application
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ app.py                  # FastAPI application
â”‚       â”œâ”€â”€ database.py             # SQLite operations
â”‚       â”œâ”€â”€ __pycache__/            # Compiled bytecode
â”‚       â””â”€â”€ static/
â”‚           â”œâ”€â”€ index.html          # Web UI (modern interface)
â”‚           â””â”€â”€ empty.html          # Placeholder
â”‚
â”œâ”€â”€ ğŸ§ª Testing
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ conftest.py             # Pytest configuration
â”‚   â”‚   â”œâ”€â”€ test_executor.py        # Executor tests
â”‚   â”‚   â”œâ”€â”€ test_persistence.py     # Persistence tests
â”‚   â”‚   â”œâ”€â”€ test_planner.py         # Planner tests
â”‚   â”‚   â””â”€â”€ test_runner.py          # Runner tests
â”‚   â””â”€â”€ pytest.ini                  # Test configuration
â”‚
â”œâ”€â”€ ğŸ“‚ Environment
â”‚   â”œâ”€â”€ .venv/                      # Python virtual environment (created by you)
â”‚   â””â”€â”€ .git/                       # Git repository
â”‚
â””â”€â”€ ğŸ“‚ Preview (Unused)
    â””â”€â”€ preview/
        â””â”€â”€ landing.html            # Legacy preview file
```

---

## ğŸš€ Usage Guide

### Scenario 1: Start Web Server
```bash
# Setup (first time only)
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your API key
nano .env

# Start server
./start_server.sh

# Visit http://127.0.0.1:8000 in browser
```

---

### Scenario 2: Run Interactive Demo
```bash
# Make sure .venv and .env are set up (from Scenario 1)

# Run full demo
./demo.sh

# Demo will:
# âœ… Check environment
# âœ… Start server
# âœ… Run example workflows
# âœ… Show metrics
# âœ… Open web UI in browser
# âœ… Test CLI
# âœ… Cleanup automatically
```

---

### Scenario 3: CLI with State Persistence
```bash
# Start with persist flag
python -m agent.runner --mode confirm --persist

# You'll be prompted:
# Enter your goal: "Create a landing page"
# [Tasks execute and save to state.json]

# If interrupted:
# Press Ctrl+C to stop

# Run again - state.json is reloaded:
python -m agent.runner --mode confirm --persist
# [Previous tasks shown, can resume or start over]
```

---

### Scenario 4: Programmatic API Access
```bash
# Start server (from Scenario 1)
./start_server.sh

# In another terminal, execute a run
curl -X POST http://127.0.0.1:8000/run \
  -H "Content-Type: application/json" \
  -d '{
    "goal": "Create a Python script",
    "max_steps": 5,
    "mode": "auto",
    "model": "gpt-4.1-mini"
  }'

# Get run history
curl http://127.0.0.1:8000/runs

# Get specific run details
curl http://127.0.0.1:8000/run/{run_id}
```

---

### Scenario 5: Configuration & Secrets
```bash
# Option 1: Use .env file (Recommended)
cp .env.example .env
echo "OPENAI_API_KEY=sk-proj-your-actual-key" >> .env
./start_server.sh

# Option 2: Set environment variable (CI/CD)
export OPENAI_API_KEY="sk-proj-your-actual-key"
./start_server.sh

# Option 3: Pass via command (Not recommended for secrets)
OPENAI_API_KEY="sk-proj-your-key" python -m uvicorn server.app:app
```

---

## ğŸ“‹ Quick Reference

| File | Type | Purpose | Created By | Status |
|------|------|---------|------------|--------|
| `.env` | Config | API key storage | You (from .env.example) | Required |
| `.env.example` | Template | Config template | Project | Reference only |
| `start_server.sh` | Shell script | Quick server start | Project | Ready to use |
| `demo.sh` | Shell script | Interactive demo | Project | Ready to use |
| `state.json` | JSON | CLI execution history | CLI with --persist | Auto-created |
| `agent_runs.db` | SQLite DB | Web run history | Web server | Auto-created |
| `.gitignore` | Config | Git ignore rules | Project | Built-in |

---

## ğŸ” Security Best Practices

### API Keys
- âœ… Store in `.env` (never in code)
- âœ… Use `.env.example` as template
- âœ… Rotate keys periodically
- âŒ Never commit `.env` to Git
- âŒ Never hard-code keys in scripts
- âŒ Never share API keys

### Data Files
- âœ… Review `.gitignore` to prevent leaks
- âœ… Back up `state.json` if important
- âœ… Keep `agent_runs.db` secure
- âŒ Don't share database with credentials

### Scripts
- âœ… Make scripts executable: `chmod +x script.sh`
- âœ… Review script contents before running
- âœ… Don't run scripts from untrusted sources
- âŒ Don't modify production scripts without testing

---

## ğŸ†˜ Troubleshooting

### ".env file not found"
**Solution**:
```bash
cp .env.example .env
nano .env  # Add your API key
```

### "Virtual environment not found"
**Solution**:
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### "Port 8000 already in use"
**Solution**:
```bash
# Use different port
python -m uvicorn server.app:app --host 127.0.0.1 --port 8001

# Or kill process using 8000
lsof -ti:8000 | xargs kill -9  # macOS/Linux
```

### "state.json keeps growing"
**Solution**:
```bash
# Backup old state
cp state.json state.json.backup

# Reset state
rm state.json

# Next run creates fresh state.json
python -m agent.runner --persist
```

### Database error
**Solution**:
```bash
# Reset database
rm agent_runs.db

# Restart server - creates new database
./start_server.sh
```

---

## ğŸ“– See Also

- **[README.md](README.md)** - Project overview and quick start
- **[RUNNING_INSTRUCTIONS.md](RUNNING_INSTRUCTIONS.md)** - Complete setup guide
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System design and scalability
- **[API.md](API.md)** - API reference and examples
- **[FEATURES.md](FEATURES.md)** - Feature showcase

---

## Next Steps

1. âœ… Review this guide
2. ğŸ“ Set up `.env` with your API key
3. ğŸš€ Run `./start_server.sh` or `./demo.sh`
4. ğŸŒ Visit `http://127.0.0.1:8000` in browser
5. ğŸ“š Check other documentation files for detailed info

**Happy automating!** ğŸ¤–
